Terraform:
It is the process of managing and provisioning the complete IT infrastructure (comprises both physical and virtual machines) using machine-readable definition files. It is a software engineering approach toward operations. It helps in automating the complete data center by using programming scripts.
Terraform's primary function is to create, modify, and destroy infrastructure resources to match the desired state described in a Terraform configuration.
Below are some of the benefits of using Terraform.
•	Does orchestration, not just configuration management
•	Supports multiple providers such as AWS, Azure, GCP, DigitalOcean and many more
•	Provide immutable infrastructure where configuration changes smoothly
•	Uses easy to understand language, HCL (HashiCorp configuration language)
•	Easily portable to any other provider
•	Supports Client only architecture, so no need for additional configuration management on a server
•	Terraform is easy to integrate with configuration management tools like Ansible.
•	It is easily extensible with plugins. 
•	It is free.

 
•	Terraform init initializes the working directory which consists of all the configuration files
•	Terraform plan is used to create an execution plan to reach a desired state of the infrastructure. Changes in the configuration files are done in order to achieve the desired state.
•	Terraform apply then makes the changes in the infrastructure as defined in the plan, and the infrastructure comes to the desired state.
•	Terraform destroy is used to delete all the old infrastructure resources, which are marked tainted after the apply phase.
•	terraform destroy -target aws_instance.my_ec2

Terraform Core concepts
Below are the core concepts/terminologies used in Terraform:
•	Variables: Also used as input-variables, it is key-value pair used by Terraform modules to allow customization.
•	Instead of repeating the IP value over and over again. When you first need it you can put the value in a central source from which you can import values in Terraform projects. Now if you needed to update the value you can just update it in the central source and it will update in all your projects.
•	File extension - *.tfvars
•	This is often used in production environments to have one config file that can be applied to multiple environments (UAT, PreProd, Prod). You would have your one config file and in order to provision or make changes to Prod you would pass the prod.tfvars file.
•	
•	The command line syntax for this is as follows:
•	terraform apply -var-file="prod.tfvars"
•	Best Practice for variables in production is to define defaults in a variables.tf file and then put any necessary overrides in a terraform.tfvars file.
•	Provider: It is a plugin to interact with APIs of service and access its related resources.
•	Module: It is a folder with Terraform templates where all the configurations are defined
•	State: It consists of cached information about the infrastructure managed by Terraform and the related configurations.
•	Resources: It refers to a block of one or more infrastructure objects (compute instances, virtual networks, etc.), which are used in configuring and managing the infrastructure.
•	Data Source: It is implemented by providers to return information on external objects to terraform.
•	Output Values: These are return values of a terraform module that can be used by other configurations.
•	Plan: It is one of the stages where it determines what needs to be created, updated, or destroyed to move from real/current state of the infrastructure to the desired state.
•	Apply: It is one of the stages where it applies the changes real/current state of the infrastructure to move to the desired state.
	File extension for terraform file is *.tf

How Terraform Works?
Terraform has two main components that make up its architecture:
•	Terraform Core
•	Providers
 
Terraform Core
Terraform core uses two input sources to do its job.
The first input source is a Terraform configuration that you, as a user, configure. Here, you define what needs to be created or provisioned. And the second input source is a state where terraform keeps the up-to-date state of how the current set up of the infrastructure looks like.
So, what terraform core does is it takes the input, and it figures out the plan of what needs to be done. It compares the state, what is the current state, and what is the configuration that you desire in the end result. It figures out what needs to be done to get to that desired state in the configuration file. It figures what needs to be created, what needs to be updated, what needs to be deleted to create and provision the infrastructure.
Providers
The second component of the architecture are providers for specific technologies. This could be clouding providers like AWS, Azure, GCP, or other infrastructure as a service platform. It is also a provider for more high-level components like Kubernetes or other platform-as-a-service tools, even some software as a self-service tool.
It gives you the possibility to create infrastructure on different levels.
For example – create an AWS infrastructure, then deploy Kubernetes on top of it and then create services/components inside that Kubernetes cluster.
Terraform has over a hundred providers for different technologies, and each provider then gives terraform user access to its resources. So through AWS provider, for example, you have access to hundreds of AWS resources like EC2 instances, the AWS users, etc. With Kubernetes provider, you access to commodities, resources like services and deployments and namespaces, etc.
So, this is how Terraforming works, and this way, it tries to help you provision and cover the complete application setup from infrastructure all the way to the application.

Dependency Lock File:
The Terraform dependency lock file (terraform.lock.hcl) allows us to lock to a specific version of the provider. 

You can override that behaviour by adding  upgrade when you run terraform init
 You can use the terraform fmt command to rewrite your Terraform configuration files with best practice formatting.
 terraform validate checks whether a configuration file is syntactically valid. It can check for issues including unsupported arguments, undeclared variables etc.

Difference:
•	Ansible, Chef and Puppet are configuration management tools which means they are primarily designed to install and manage software on existing servers
•	Terraform and CloudFormation are infrastructure orchestration tools which are designed to provision servers and infrastructure themselves.

Ref - Ultimate Guide to Terraform: From Beginner to Expert — Zero to Devops


Ansible 

Ansible can be used to deploy the software on different servers at a time without human interaction. 
Ansible can also be used to configure the servers and create user accounts. Ansible is an agent-less software which means there is no need to install the software in the nodes which means you need to do the SSH to connect the nodes to perform the required operations on the servers.
Ansible playbooks can be written very easily they will be like plain english and Ansible is developed using Python language. There is no need for any knowledge of programming. A single ansible control node can manage thousands of the nodes.

Ansible is entirely agentless, which means Ansible works by connecting your nodes through SSH (by default). Ansible gives the option to you if you want another method for the connection like Kerberos.

Ansible Architecture
1.	Control node: Commands and Playbooks can run by invoking /usr/bin/ansible or /usr/bin/ansible-playbook, from any control node. You can use any computer that has Python installed on it as a control node. However, one can not use a computer with Windows OS as a control node. One can have multiple control nodes.
2.	Managed nodes: Also sometimes called “hosts”, Managed nodes are the network devices (and/or servers) you manage with Ansible.
3.	Inventory: Also sometimes called “hostfile”, Inventory is the list of Managed nodes use to organize them. It is also used for creating and nesting groups for easier scaling.
4.	Modules: These are the units of code executed by Ansible. Each module can be used for a specific purpose. One can invoke a single module with a task, or invoke several different modules in a playbook.
5.	Tasks: The units of action in Ansible. One can execute a single task once with an ad-hoc command.

Ansible Workflow

Ansible works by connecting to your nodes and pushing out a small program called Ansible modules to them. Then Ansible executed these modules and removed them after finished. The library of modules can reside on any machine, and there are no daemons, servers, or databases required.



 

Playbook Structure
Each playbook is a collection of one or more plays. Playbooks are structured by using Plays. There can be more than one play inside a playbook.
 
The function of the play is to map a set of instructions which is defined against a particular host.

